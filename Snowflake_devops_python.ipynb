{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREREQUISTE\n",
    "\n",
    "1. Create Snowflake user\n",
    "Create User and update the credential as described here:\n",
    "https://docs.snowflake.com/en/developer-guide/snowflake-cli-v2/connecting/specify-credentials\n",
    "\n",
    "use role sysadmin and makesure you grant following\n",
    "\n",
    "GRANT EXECUTE TASK ON ACCOUNT TO ROLE SYSADMIN;\n",
    "\n",
    "2. Create SnowCLI utility - for authentication only\n",
    "https://docs.snowflake.com/en/developer-guide/snowflake-cli-v2/installation/installation\n",
    "\n",
    "3. Create Python environment\n",
    "\n",
    "\n",
    "conda create --name snow_env --override-channels -c https://repo.anaconda.com/pkgs/snowflake python=3.10 numpy pandas pyarrow\n",
    "\n",
    "conda activate snow_env\n",
    "\n",
    "\n",
    "conda install snowflake-snowpark-python\n",
    "\n",
    "\n",
    "pip install snowflake - U\n",
    "\n",
    "\n",
    "pip install ploty\n",
    "\n",
    "4. run in Visual code or your choice of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snowflake python API packages\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.database import Database\n",
    "from snowflake.core.schema import Schema\n",
    "from snowflake.core.table import Table, TableColumn, PrimaryKey\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core.role import Role\n",
    "from snowflake.core.grant import Grant\n",
    "from snowflake.core.grant import Grant, Grantees, Privileges, Securables\n",
    "from snowflake.core.stage import Stage\n",
    "from snowflake.core.stage import StageCollection\n",
    "from snowflake.core.task import StoredProcedureCall, Task\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit\n",
    "from snowflake.snowpark.version import VERSION\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark.functions import sproc\n",
    "#from snowflake.snowpark import Window\n",
    "\n",
    "# Import Python packages\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.config_manager import CONFIG_MANAGER\n",
    "# file from connection is created\n",
    "print(CONFIG_MANAGER.file_path)\n",
    "# https://docs.snowflake.com/en/developer-guide/snowflake-cli-v2/connecting/specify-credentials\n",
    "# location in mac ~/Library/Application Support/snowflake/config.toml\n",
    "# following take default connection from above file\n",
    "#session = Session.builder.getOrCreate()\n",
    "# following for specific connection\n",
    "session=Session.builder.config(\"connection_name\",\"demodc\").create()\n",
    "snowpark_version = VERSION\n",
    "# Current Environment Details\n",
    "print('Account                     : {}'.format(session.get_current_account()))\n",
    "print('User                        : {}'.format(session.get_current_user()))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "\n",
    "root = Root(session) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database\n",
    "tb101_db = root.databases.create(Database(name=\"tb_101\"),  mode=\"ifNotExists\") # orReplace, errorIfExists \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create schemas\n",
    "raw_pos_sch = tb101_db.schemas.create(Schema(name=\"raw_pos\"), mode=\"orReplace\")\n",
    "raw_cust_sch = tb101_db.schemas.create(Schema(name=\"raw_customer\"), mode=\"orReplace\")\n",
    "harmonized_sch = tb101_db.schemas.create(Schema(name=\"harmonized\"), mode=\"orReplace\")\n",
    "analytics_sch = tb101_db.schemas.create(Schema(name=\"analytics\"), mode=\"orReplace\")\n",
    "session.use_schema('raw_pos')\n",
    "\n",
    "# list schemas\n",
    "# SQL  show schemas in database \"tb_101\";\n",
    "schema_collection = root.databases[\"tb_101\"].schemas.iter()\n",
    "for sch in schema_collection:\n",
    "  print(sch.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create warehouse\n",
    "# create a Warehouse instance that used to store the property of a warehouse\n",
    "warehouses = root.warehouses\n",
    "warehouse_name = \"tb_de_wh\"\n",
    "de_wh= Warehouse(\n",
    "    name=warehouse_name,\n",
    "    warehouse_size=\"LARGE\",\n",
    "    auto_suspend=60,\n",
    "    initially_suspended=\"true\",\n",
    "    comment=\"data engg warehouse for tasty bytes\",\n",
    "    auto_resume=\"true\"\n",
    ")\n",
    "dev_wh= Warehouse(\n",
    "    name=\"tb_dev_wh\",\n",
    "    warehouse_size=\"XSMALL\",\n",
    "    auto_suspend=60,\n",
    "    initially_suspended=\"true\",\n",
    "    comment=\"developer warehouse for tasty bytes\",\n",
    "    auto_resume=\"true\"\n",
    ")\n",
    "# create a warehouse \n",
    "\n",
    "dewh  = warehouses.create(de_wh, mode='orReplace')\n",
    "devwh = warehouses.create(dev_wh,mode='orReplace')\n",
    "\n",
    "session.use_warehouse('tb_de_wh')\n",
    "\n",
    "# list warehosues\n",
    "# SQL  show warehouses like '%demo%';\n",
    "warehouse_collection = warehouses.iter(like=\"tb%\")\n",
    "for wh in warehouse_collection:\n",
    "  print(wh.name, wh.auto_suspend, wh.max_cluster_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create roles\n",
    "session.use_role(\"securityadmin\")\n",
    "tbadmin_role = Role(name=\"tb_admin\",  comment='admin for tasty bytes')\n",
    "tbadmin_role = root.roles.create(tbadmin_role,  mode='ifNotExists')\n",
    "tbde_role = Role(name=\"tb_data_engineer\",  comment='data engineer for tasty bytes')\n",
    "tbde_role = root.roles.create(tbde_role,  mode='ifNotExists')\n",
    "tbdev_role = Role(name=\"tb_dev\",  comment='developer for tasty bytes')\n",
    "tbdev_role = root.roles.create(tbdev_role,  mode='ifNotExists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give a grant to create hierarchey , tb_dev-> tb_data_engineer-> tb_admin -> sysadmin\n",
    "# GRANT ROLE TB_ADMIN TO ROLE SYSADMIN\n",
    "root.grants.grant(\n",
    "  Grant(\n",
    "    grantee=Grantees.role('sysadmin'), \n",
    "    securable=Securables.role('tb_admin'),\n",
    "  )\n",
    ")\n",
    "# GRANT ROLE TB_DATA_ENGINEER TO ROLE TB_ADMIN\n",
    "root.grants.grant(\n",
    "  Grant(\n",
    "    grantee=Grantees.role('tb_admin'), \n",
    "    securable=Securables.role('tb_data_engineer'),\n",
    "  )\n",
    ")\n",
    "# GRANT ROLE TB_DEV TO ROLE TB_DATA ENGINEER\n",
    "root.grants.grant(\n",
    "  Grant(\n",
    "    grantee=Grantees.role('tb_data_engineer'), \n",
    "    securable=Securables.role('tb_dev'),\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usages grants\n",
    "# GRANT USAGE ON DATABASE TB_101 TO ROLE TB_DEV\n",
    "\n",
    "root.grants.grant(\n",
    "  Grant(\n",
    "    grantee=Grantees.role('tb_dev'),\n",
    "    securable=Securables.database('tb_101'),\n",
    "    privileges=[Privileges.usage]\n",
    "  )\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stage\n",
    "#CREATE STAGE IF NOT EXISTS  tb_101.public.s3load_csv  URL = 's3://sfquickstarts/frostbyte_tastybytes/'      \n",
    "session.use_role(\"sysadmin\")\n",
    "s3_stage_csv = Stage(\n",
    "  name=\"s3load_csv\", url='s3://sfquickstarts/frostbyte_tastybytes/'\n",
    ")\n",
    "stages = root.databases[\"tb_101\"].schemas[\"public\"].stages\n",
    "stages.create(s3_stage_csv, mode='ifNotExists')\n",
    "\n",
    "# this stage to use to store python procedure code so it can execute later in task\n",
    "int_stage = Stage(name=\"mycode\")\n",
    "stages.create(int_stage, mode='ifNotExists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the stage\n",
    "# LIST @TB_101.PUBLIC.S3LOAD_CSV;\n",
    "s3files = root.databases[\"tb_101\"].schemas[\"public\"].stages[\"s3load_csv\"].list_files()\n",
    "for stageFile in s3files:\n",
    "  print(stageFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and create table and load data from above stage\n",
    "shift_sales = Table(\n",
    "  name=\"shift_sales\",\n",
    "  columns=[TableColumn(name=\"location_id\", datatype=\"number(38,0)\"),\n",
    "           TableColumn(name=\"city\", datatype=\"string\"),\n",
    "           TableColumn(name=\"date\", datatype=\"date\"),\n",
    "           TableColumn(name=\"shift_sales\", datatype=\"float\"),\n",
    "           TableColumn(name=\"shift\", datatype=\"string\"),\n",
    "           TableColumn(name=\"month\", datatype=\"number(2,0)\"),\n",
    "           TableColumn(name=\"day_of_week\", datatype=\"number(2,0)\"),\n",
    "           TableColumn(name=\"city_population\", datatype=\"number(38,0)\"),       \n",
    "           ]\n",
    ")\n",
    "root.databases[\"tb_101\"].schemas[\"raw_pos\"].tables.create(shift_sales,mode='orReplace')\n",
    "\n",
    "\n",
    "shift_sales_schema = StructType([\n",
    "StructField(\"location_id\",StringType()),\n",
    "StructField(\"city\",StringType()),\n",
    "StructField(\"date\",StringType()),\n",
    "StructField(\"shift_sales\",StringType()),\n",
    "StructField(\"shift\",StringType()),\n",
    "StructField(\"month\",StringType()),\n",
    "StructField(\"day_of_week\", StringType()),\n",
    "StructField(\"city_population\", StringType()),\n",
    "\n",
    "])\n",
    "shift_sales_df=session.read.options({\"field_delimiter\": \",\", \"skip_header\": 0}).schema(shift_sales_schema).csv(\"@tb_101.public.s3load_csv/analytics/shift_sales/\")\n",
    "copy_result = shift_sales_df.copy_into_table(\"shift_sales\", force=True)\n",
    "print(copy_result)\n",
    "session.table(\"shift_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tables\n",
    "tables = root.databases[\"tb_101\"].schemas[\"raw_pos\"].tables.iter(like=\"%\")\n",
    "for table_obj in tables:\n",
    "  print(table_obj.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change warehouse size to xsmall\n",
    "\n",
    "whobj=root.warehouses[\"tb_de_wh\"].fetch()\n",
    "whobj.warehouse_size=\"xsmall\"\n",
    "whobj.resource_monitor = None\n",
    "root.warehouses[\"tb_de_wh\"].create_or_update(whobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref = root.databases['tb_101'].schemas['raw_pos'].tables['shift_sales']\n",
    "demo_table = table_ref.fetch()\n",
    "demo_table.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join an create view\n",
    "shift_sales_df = session.table(\"tb_101.raw_pos.shift_sales\")\n",
    "shift_sales_df.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create database from Snowflake Marketplace called \"frostbyte_safegraph\" and grant access to the role you are using (eg. sysadmin)\n",
    "\n",
    " https://app.snowflake.com/marketplace/listing/GZSNZL1CN82/safegraph-safegraph-frostbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tb_safegraph_df=session.table('frostbyte_safegraph.public.frostbyte_tb_safegraph_s')\n",
    "tb_safegraph_df.count();\n",
    "\n",
    "tb_safegraph_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a view in analytics schema\n",
    "\n",
    "shift_sales_harmoized_df = shift_sales_df.join(tb_safegraph_df, shift_sales_df.col(\"LOCATION_ID\") == tb_safegraph_df.col(\"LOCATION_ID\"))\\\n",
    "        .select(shift_sales_df[\"LOCATION_ID\"].as_(\"LOCATION_ID\"),\n",
    "                shift_sales_df[\"CITY\"].as_(\"CITY\"),\n",
    "                shift_sales_df[\"DATE\"],\n",
    "                shift_sales_df[\"SHIFT_SALES\"],\n",
    "                shift_sales_df[\"SHIFT\"],\n",
    "                shift_sales_df[\"MONTH\"],\n",
    "                shift_sales_df[\"DAY_OF_WEEK\"],\n",
    "                shift_sales_df[\"CITY_POPULATION\"],\n",
    "                tb_safegraph_df[\"LATITUDE\"],\n",
    "                tb_safegraph_df[\"LONGITUDE\"]\n",
    "        )\n",
    "\n",
    "shift_sales_harmoized_df.show()\n",
    "shift_sales_harmoized_df.create_or_replace_view(\"TB_101.ANALYTICS.SHIFT_SALES_VW\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df = session.table(\"tb_101.analytics.shift_sales_vw\")\n",
    "ss_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stage to load data from  parquet files\n",
    "s3_stage_parquet = Stage(\n",
    "  name=\"s3load_parquet\", url='s3://sfquickstarts/data-engineering-with-snowpark-python/'\n",
    ")\n",
    "stages_parquet = root.databases[\"tb_101\"].schemas[\"public\"].stages\n",
    "stages_parquet.create(s3_stage_parquet, mode='orReplace')\n",
    "\n",
    "# List files in the parquet stage\n",
    "# LIST @TB_101.PUBLIC.s3load_parquet;\n",
    "s3files = root.databases[\"tb_101\"].schemas[\"public\"].stages[\"s3load_parquet\"].list_files(pattern=\".*truck.*\")\n",
    "for stageFile in s3files:\n",
    "  print(stageFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete table truck using schema detection\n",
    "stage_loc= \"@tb_101.public.s3load_parquet/pos/truck/\"\n",
    "truckdf = session.read.option(\"compression\", \"snappy\").parquet(stage_loc)\n",
    "tref=truckdf.copy_into_table(\"truck\")\n",
    "print(tref)\n",
    "table_ref = root.databases['tb_101'].schemas['raw_pos'].tables['truck']\n",
    "truck_table = table_ref.fetch()\n",
    "truck_table.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_df = session.table(\"tb_101.raw_pos.truck\")\n",
    "\n",
    "print(truck_df.count())\n",
    "shift_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sproc(name='tb_101.raw_pos.ingest_data_sproc', \n",
    "       packages=['snowflake-snowpark-python'], \n",
    "       is_permanent=True, \n",
    "       replace=True,\n",
    "       stage_location='@tb_101.public.mycode', \n",
    "       session=session)\n",
    "def ingest_data_sproc(session: Session) -> T.Variant:\n",
    "       shift_sales_schema = StructType([\n",
    "              StructField(\"location_id\",StringType()),\n",
    "              StructField(\"city\",StringType()),\n",
    "              StructField(\"date\",StringType()),\n",
    "              StructField(\"shift_sales\",StringType()),\n",
    "              StructField(\"shift\",StringType()),\n",
    "              StructField(\"month\",StringType()),\n",
    "              StructField(\"day_of_week\", StringType()),\n",
    "              StructField(\"city_population\", StringType()),\n",
    "\n",
    "       ])\n",
    "       shift_sales_df=session.read.options({\"field_delimiter\": \",\", \"skip_header\": 0}).schema(shift_sales_schema).csv(\"@tb_101.public.s3load_csv/analytics/shift_sales/\")\n",
    "       copy_result = shift_sales_df.copy_into_table(\"shift_sales\", force=True)\n",
    "       stage_loc= \"@tb_101.public.s3load_parquet/pos/truck/\"\n",
    "       truckdf = session.read.option(\"compression\", \"snappy\").parquet(stage_loc)\n",
    "       tref=truckdf.copy_into_table(\"truck\")\n",
    "       return (\"Data loading complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sproc(name='tb_101.raw_pos.change_wh_size', \n",
    "       packages=['snowflake-snowpark-python','snowflake.core'], \n",
    "       is_permanent=True, \n",
    "       replace=True,\n",
    "       stage_location='@tb_101.public.mycode', \n",
    "       session=session)\n",
    "def change_wh_size(session: Session, whname: str, whsize: str) -> T.Variant:\n",
    "       root = Root(session) \n",
    "       whobj=root.warehouses[whname].fetch()\n",
    "       whobj.warehouse_size=whsize\n",
    "       whobj.resource_monitor = None\n",
    "       root.warehouses[whname].create_or_update(whobj)\n",
    "       return (\"Warehouse Size changed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Task instance in the client side\n",
    "#serverless task\n",
    "increase_whsize_task = Task(\n",
    "    \"increase_wh_size\",\n",
    "    definition=StoredProcedureCall(change_wh_size, args=[\"tb_de_wh\",\"xlarge\"], stage_location=\"@tb_101.public.mycode\", packages=[\"snowflake-snowpark-python\",\"snowflake.core\"]),    \n",
    "    warehouse=\"tb_de_wh\",\n",
    "    schedule=timedelta(hours=1)\n",
    ")\n",
    "ingest_task = Task(\n",
    "    \"loaddata_task\",\n",
    "    definition=StoredProcedureCall(ingest_data_sproc, stage_location=\"@tb_101.public.mycode\", packages=[\"snowflake-snowpark-python\"]),\n",
    "    warehouse=\"tb_de_wh\"\n",
    ")\n",
    "reduce_whsize_task = Task(\n",
    "    \"reduce_wh_size\",\n",
    "    definition=\"call tb_101.raw_pos.change_wh_size('tb_de_wh','xsmall')\" ,\n",
    "    user_task_managed_initial_warehouse_size=\"xsmall\"\n",
    ")\n",
    "# create chain of task/DAG\n",
    "reduce_whsize_task.predecessors = [ingest_task.name]\n",
    "ingest_task.predecessors = [increase_whsize_task.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = root.databases[\"tb_101\"].schemas[\"public\"].tasks\n",
    "task0 = tasks.create(increase_whsize_task, mode=\"orreplace\")\n",
    "task1 = tasks.create(ingest_task, mode=\"orreplace\")\n",
    "task2 = tasks.create(reduce_whsize_task, mode=\"orreplace\")\n",
    "for t in tasks.iter(like=\"%\"):\n",
    "    print(f\"Definition of {t.name}: \\n\",  t.definition, sep=\"\", end=\"\\n--------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task2.resume()\n",
    "#task1.resume()\n",
    "#task0.resume()\n",
    "task0.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task0.get_current_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task0.suspend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. OR Create a DAG\n",
    "\n",
    "dag_name = \"data_ingestion_dag\" \n",
    "dag = DAG(dag_name, schedule=timedelta(hours=1))\n",
    "with dag:\n",
    "    dag_task1 = DAGTask(\"increase_wh_size\",\n",
    "           definition=StoredProcedureCall(change_wh_size, args=[\"tb_de_wh\",\"xlarge\"], stage_location=\"@tb_101.public.mycode\", \n",
    "                    packages=[\"snowflake-snowpark-python\",\"snowflake.core\"]),  warehouse=\"tb_de_wh\")    \n",
    "    dag_task2 = DAGTask(\"loaddata_task\", StoredProcedureCall(ingest_data_sproc, stage_location=\"@tb_101.public.mycode\", packages=[\"snowflake-snowpark-python\"]), warehouse=\"tb_de_wh\")\n",
    "    dag_task3 = DAGTask(\"reduce_wh_size\",definition=StoredProcedureCall(change_wh_size, args=[\"tb_de_wh\",\"xsmall\"], stage_location=\"@tb_101.public.mycode\", \n",
    "                    packages=[\"snowflake-snowpark-python\",\"snowflake.core\"]),  warehouse=\"tb_de_wh\")\n",
    "    dag_task1 >> dag_task2 >> dag_task3  # task1 is a predecessor of task2 which is predecssor of task3\n",
    "pubschema = root.databases[\"tb_101\"].schemas[\"public\"]\n",
    "dag_op = DAGOperation(pubschema)\n",
    "dag_op.deploy(dag, mode=\"orreplace\")  \n",
    "\n",
    "dagiter = dag_op.iter_dags(like='%')\n",
    "for dag_name in dagiter:\n",
    "    print(dag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_op.run(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation and analytics\n",
    "# Select\n",
    "location_df = ss_df.select(\"date\", \"shift\", \"shift_sales\", \"location_id\", \"city\")\n",
    "\n",
    "# Filter\n",
    "location_df = location_df.filter(F.col(\"location_id\") == 1135)\n",
    "\n",
    "# Sort\n",
    "location_df = location_df.order_by([\"date\", \"shift\"], ascending=[0, 0])\n",
    "\n",
    "# Display\n",
    "location_df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift sales table\n",
    "print(location_df.count())\n",
    "location_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and average shift sales\n",
    "analysis_df = ss_df.group_by(\"city\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n",
    "\n",
    "# Sort by average shift sales\n",
    "analysis_df = analysis_df.sort(\"avg_shift_sales\", ascending=True)\n",
    "\n",
    "# Pull to pandas and plot\n",
    "analysis_df.to_pandas().plot.barh(x=\"CITY\", y=\"AVG_SHIFT_SALES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Vancouver\n",
    "analysis_df = ss_df.filter(F.col(\"city\") == \"Vancouver\")\n",
    "\n",
    "# Group by location and average shift sales\n",
    "analysis_df = analysis_df.group_by(\"location_id\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n",
    "\n",
    "# Get the location count\n",
    "print(\"Vancouver location count:\", analysis_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = analysis_df.to_pandas().hist(column=\"AVG_SHIFT_SALES\", bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup\n",
    "#task2.delete()\n",
    "#task1.delete()\n",
    "#task0.delete()\n",
    "#dag_op.delete(dag)\n",
    "#tb101_db.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
